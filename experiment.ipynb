{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务：基于HMM模型实现KTH视频数据的动作分类\n",
    "1.\t下载KTH数据集文件，并从数据集的视频中提取特征；奇数索引的视频数据作为训练集，偶数索引的视频数据作为测试集。\n",
    "##### 实验1\n",
    "2.\t在提取特征后的训练数据的基础上，实现一个空间聚类算法（例如 K-Means，任何聚类算法均可）。\n",
    "3.\t为每类动作训练单独的隐马尔可夫模型（每个元音单独聚类，然后分别学习 HMM，输入是与每个2D点关联的聚类类别号）。对于每个测试数据，针对每个HMM计算其对数似然，即log P(O|M)，并获取给出最高对数似然的HMM类别，即对测试数据进行分类 判别。给出混淆矩阵并描述你的发现。\n",
    "4.\t改变2中的聚类数量（变量M）和3中隐藏节点的数量（变量N）并计算分类准确率。给出不同M和N取值下的混淆矩阵，并描述你的发现。\n",
    "##### 实验2\n",
    "5.\t实现动态时间规整算法（Dynamic Time Warping），并重复1-4。\n",
    "##### 实验3\n",
    "6.\t学习一个HMM模型，该HMM可以使用维特比解码(Viterbi Decoding)来执行分类（注意：本题目不是让你生成多个分类器并进行分类判别，而是生成一个HMM模型，可以执行多个类别判别）。将分类准确率与题目 3 和4的结果进行比较，并描述你的发现。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from hmmlearn import hmm\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dt_f(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    dense_sampling_interval = frame_rate\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_frames = min(frame_count, frame_rate * 100)  \n",
    "    dense_trajectories = []\n",
    "    prev_gray = None\n",
    "    frame_number = 0\n",
    "    hsv = np.zeros((160, 120, 3))\n",
    "    while frame_number < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (120, 160))\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if prev_gray is not None:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "            hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            dense_trajectories.append(hsv.flatten())\n",
    "        \n",
    "        prev_gray = gray\n",
    "        frame_number += dense_sampling_interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(dense_trajectories),np.array(dense_trajectories).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取特征的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: (16, 57600)\n",
      "[[ 1.79402969e+02  0.00000000e+00  6.97106495e-02 ...  1.33374924e+02\n",
      "   0.00000000e+00  1.02969337e+00]\n",
      " [ 8.76478500e+01  0.00000000e+00  1.28269106e-01 ...  3.30666199e+01\n",
      "   0.00000000e+00  2.07341144e-10]\n",
      " [ 7.53964996e+00  0.00000000e+00  3.57635784e+00 ...  1.70400116e+02\n",
      "   0.00000000e+00 -1.43300571e-09]\n",
      " ...\n",
      " [ 9.47653046e+01  0.00000000e+00  1.15680850e+00 ...  8.50442047e+01\n",
      "   0.00000000e+00  2.57939649e+00]\n",
      " [ 1.10593462e+01  0.00000000e+00  8.45990628e-02 ...  1.03541100e+02\n",
      "   0.00000000e+00  3.94400731e-02]\n",
      " [ 9.13240356e+01  0.00000000e+00  7.83343017e-02 ...  6.04778633e+01\n",
      "   0.00000000e+00  1.43436951e-09]]\n"
     ]
    }
   ],
   "source": [
    "from datasets import extract_features_dt_f\n",
    "# Example usage:\n",
    "video_path = r\"data\\jogging\\person01_jogging_d1_uncomp.avi\"  \n",
    "features,_ = extract_features_dt_f(video_path)\n",
    "features = np.array(features)\n",
    "print(f'Shape of features: {features.shape}')\n",
    "num_frames=features.shape[0]\n",
    "print(features)\n",
    "n_features=features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "#---------------------dt-----------------------\n",
    "with open('Data/data_dt_f.pkl', 'rb') as f:\n",
    "     data = pickle.load(f)\n",
    "# 从.pkl文件加载数据\n",
    "with open('Data/labels_dt.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "with open('Data/frames.pkl', 'rb') as f:\n",
    "     frames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 57600)\n"
     ]
    }
   ],
   "source": [
    "# print(data_pad[103].shape)\n",
    "print(data[399].shape)\n",
    "#num_frames=data[399].shape[0]\n",
    "n_features=data[399].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 57600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 57600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(data)==599\n",
    "print(data[103].shape)\n",
    "data[590].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\顾宇豪\\AppData\\Local\\Temp\\ipykernel_22692\\264286344.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data=np.array(data)\n"
     ]
    }
   ],
   "source": [
    "data=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[209])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将各个动作视频分开，因为后续要单独聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "actions = [\"walking\", \"boxing\", \"handclapping\", \"jogging\", \"running\", \"handwaving\"]\n",
    "data_walk=data[0:100]\n",
    "data_box=data[100:200]\n",
    "data_handclap=data[200:299]\n",
    "data_jog=data[299:399]\n",
    "data_run=data[399:499]\n",
    "data_wave=data[499:599]\n",
    "\n",
    "frames_walk=frames[0:100]\n",
    "frames_box=frames[100:200]\n",
    "frames_handclap=frames[200:299]\n",
    "frames_jog=frames[299:399]\n",
    "frames_run=frames[399:499]\n",
    "frames_wave=frames[499:599]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = []  # 用来存储分割后的序列列表\n",
    "for i,label in enumerate(labels):\n",
    "    for j in range(frames[i]):\n",
    "        labels_.append(label)\n",
    "labels_ = np.array(labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(labels_)==11254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# 假设 data 是您的数据集，形状为 (598, 50, 15552)\n",
    "# 重新组织数据，将每个视频的帧、x坐标和y坐标合并成特征向量\n",
    "data_walk=np.vstack(data_walk)\n",
    "data_box=np.vstack(data_box)\n",
    "data_handclap=np.vstack(data_handclap)\n",
    "data_jog=np.vstack(data_jog)\n",
    "data_run=np.vstack(data_run)\n",
    "data_wave=np.vstack(data_wave)\n",
    "data=np.vstack(data)\n",
    "\n",
    "# 检查序列长度是否符合\n",
    "assert data_walk.shape[0]==sum(frames_walk)\n",
    "assert data_box.shape[0]==sum(frames_box)\n",
    "assert data_handclap.shape[0]==sum(frames_handclap)\n",
    "assert data_jog.shape[0]==sum(frames_jog)\n",
    "assert data_run.shape[0]==sum(frames_run)\n",
    "assert data_wave.shape[0]==sum(frames_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行PCA降维\n",
    "n_pca=260     #主成分个数\n",
    "pca=PCA(n_components=n_pca)\n",
    "pca.fit(data)\n",
    "data_pca_walk = pca.transform(data_walk)\n",
    "data_pca_box = pca.transform(data_box)\n",
    "data_pca_handclap= pca.transform(data_handclap)\n",
    "data_pca_jog = pca.transform(data_jog)\n",
    "data_pca_run = pca.transform(data_run)\n",
    "data_pca_wave = pca.transform(data_wave)\n",
    "data_pca=pca.transform(data)\n",
    "\n",
    "# 对每个数据点进行LDA降维\n",
    "n_lda=5\n",
    "lda =  LinearDiscriminantAnalysis(n_components=n_lda)\n",
    "lda.fit(np.concatenate([data_pca_walk,data_pca_box,data_pca_handclap,data_pca_jog,data_pca_run,data_pca_wave]),labels_)\n",
    "data_lda = lda.transform(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制PCA降维后主成分的累积可解释方差比例的曲线\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制LDA降维后的主成分累积可解释方差比例的曲线\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(lda.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对各个类别的视频进行单独聚类 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对各种视频单独进行KMeans聚类\n",
    "num_clusters = 6  \n",
    "\n",
    "kmeans_walk = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_walk = kmeans_walk.fit_predict(data_pca_walk) \n",
    "\n",
    "kmeans_box = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_box = kmeans_box.fit_predict(data_pca_box)\n",
    "\n",
    "kmeans_handclap = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_handclap = kmeans_handclap.fit_predict(data_pca_handclap) \n",
    "\n",
    "kmeans_jog = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_jog = kmeans_jog.fit_predict(data_pca_jog) \n",
    "\n",
    "kmeans_run = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_run = kmeans_run.fit_predict(data_pca_run) \n",
    "\n",
    "kmeans_wave = KMeans(n_clusters=num_clusters, random_state=0,n_init=300)\n",
    "clusters_wave = kmeans_wave.fit_predict(data_pca_wave) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验3中对所有数据进行一起聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有视频一起进行KMeans聚类\n",
    "num_clusters_=18\n",
    "kmeans = KMeans(n_clusters=num_clusters_, random_state=0,n_init=700,tol=0.00001)\n",
    "clusters=kmeans.fit_predict(data_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化聚类效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 创建一个3D图形\n",
    "fig = plt.figure(figsize=(22,9))\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "# 根据聚类标签绘制不同颜色的数据点\n",
    "for label in np.unique(clusters_walk):\n",
    "    indices = np.where(clusters_walk == label)\n",
    "    ax.scatter(data_pca_walk[indices, 0], data_pca_walk[indices, 1], data_pca_walk[indices, 2], label=f'Cluster {label}') \n",
    "    \n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('KMeans Clustering Visualization')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 创建一个3D图形\n",
    "fig = plt.figure(figsize=(22,9))\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "# 根据聚类标签绘制不同颜色的数据点\n",
    "for label in np.unique(clusters):\n",
    "    indices = np.where(clusters == label)\n",
    "    ax.scatter(data_lda[indices, 0], data_lda[indices, 1], data_lda[indices, 2], label=f'Cluster {label}') \n",
    "    \n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('KMeans Clustering Visualization')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取聚类后每个视频对应的标签序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_walk_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_walk:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_walk[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_walk_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_walk = np.array(clusters_walk_)\n",
    "\n",
    "clusters_box_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_box:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_box[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_box_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_box = np.array(clusters_box_)\n",
    "\n",
    "clusters_handclap_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_handclap:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_handclap[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_handclap_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_handclap = np.array(clusters_handclap_)\n",
    "\n",
    "clusters_jog_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_jog:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_jog[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_jog_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_jog = np.array(clusters_jog_)\n",
    "\n",
    "clusters_run_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_run:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_run[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_run_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_run= np.array(clusters_run_)\n",
    "\n",
    "clusters_wave_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames_wave:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters_wave[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_wave_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_wave = np.array(clusters_wave_)\n",
    "\n",
    "clusters_ = []  # 用来存储分割后的序列列表\n",
    "start_idx = 0\n",
    "for frame in frames:\n",
    "    # 根据frames数组中的值，从data中分割出一个序列\n",
    "    sequence = clusters[start_idx:start_idx + frame]\n",
    "    # 将该序列添加到sequences列表中\n",
    "    clusters_.append(sequence)\n",
    "    # 更新起始索引\n",
    "    start_idx += frame\n",
    "data_all = np.array(clusters_)\n",
    "\n",
    "# 输出聚类结果\n",
    "for video_index, cluster_label in enumerate(data_walk):\n",
    "    print(f\"walk视频{video_index+1}的聚类结果：{cluster_label}\")\n",
    "    \n",
    "for video_index, cluster_label in enumerate(data_box):\n",
    "    print(f\"box视频{video_index+1}的聚类结果：{cluster_label}\")\n",
    "    \n",
    "for video_index, cluster_label in enumerate(data_handclap):\n",
    "    print(f\"handclap视频{video_index+1}的聚类结果：{cluster_label}\")\n",
    "    \n",
    "for video_index, cluster_label in enumerate(data_jog):\n",
    "    print(f\"jog视频{video_index+1}的聚类结果：{cluster_label}\")\n",
    "    \n",
    "for video_index, cluster_label in enumerate(data_run):\n",
    "    print(f\"run视频{video_index+1}的聚类结果：{cluster_label}\")\n",
    "\n",
    "for video_index, cluster_label in enumerate(data_wave):\n",
    "    print(f\"wave视频{video_index+1}的聚类结果：{cluster_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split=0.5\n",
    "X_train_walk=data_walk[::2]\n",
    "X_test_walk=data_walk[1::2]\n",
    "\n",
    "X_train_box=data_box[::2]\n",
    "X_test_box=data_box[1::2]\n",
    "\n",
    "X_train_handclap=data_handclap[::2]\n",
    "X_test_handclap=data_handclap[1::2]\n",
    "\n",
    "X_train_jog=data_jog[::2]\n",
    "X_test_jog=data_jog[1::2]\n",
    "\n",
    "X_train_run=data_run[::2]\n",
    "X_test_run=data_run[1::2]\n",
    "\n",
    "X_train_wave=data_wave[::2]\n",
    "X_test_wave=data_wave[1::2]\n",
    "\n",
    "# 制作训练序列\n",
    "data_walk=np.concatenate([i for i in X_train_walk]).reshape(-1,1)\n",
    "data_box=np.concatenate([i for i in X_train_box]).reshape(-1,1)\n",
    "data_handclap=np.concatenate([i for i in X_train_handclap]).reshape(-1,1)\n",
    "data_jog=np.concatenate([i for i in X_train_jog]).reshape(-1,1)\n",
    "data_run=np.concatenate([i for i in X_train_run]).reshape(-1,1)\n",
    "data_wave=np.concatenate([i for i in X_train_wave]).reshape(-1,1)\n",
    "\n",
    "# 序列长度\n",
    "x_len_walk=frames_walk[::2]\n",
    "x_len_box=frames_box[::2]\n",
    "x_len_handclap=frames_handclap[::2]\n",
    "x_len_jog=frames_jog[::2]\n",
    "x_len_run=frames_run[::2]\n",
    "x_len_wave=frames_wave[::2]\n",
    "\n",
    "x_len_walk_=frames_walk[1::2]\n",
    "x_len_box_=frames_box[1::2]\n",
    "x_len_handclap_=frames_handclap[1::2]\n",
    "x_len_jog_=frames_jog[1::2]\n",
    "x_len_run_=frames_run[1::2]\n",
    "x_len_wave_=frames_wave[1::2]\n",
    "\n",
    "labels_walk=labels[0:100]\n",
    "labels_box=labels[100:200]\n",
    "labels_handclap=labels[200:299]\n",
    "labels_jog=labels[299:399]\n",
    "labels_run=labels[399:499]\n",
    "labels_wave=labels[499:599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_handclap))\n",
    "print(sum(x_len_handclap))\n",
    "assert len(data_walk)==sum(x_len_walk)\n",
    "assert len(data_box)==sum(x_len_box)\n",
    "assert len(data_handclap)==sum(x_len_handclap)\n",
    "assert len(data_jog)==sum(x_len_jog)\n",
    "assert len(data_run)==sum(x_len_run)\n",
    "assert len(data_wave)==sum(x_len_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练隐马尔可夫分类模型\n",
    "\n",
    "hmm中的参数：\n",
    "\n",
    "\tn_components：一个整数，指定了状态的数量。\n",
    "\tcovariance_type：一个字符串，指定了使用方差矩阵的类型。可以为：\n",
    "\t\t'spherical'：对每个状态，该状态的所有特征的方差都是同一个值。\n",
    "\t\t'diag'：每个状态的方差矩阵为对角矩阵。\n",
    "\t\t'full'：每个状态的方差矩阵为普通的矩阵。\n",
    "\t\t'tied'：所有状态都是用同一个普通的方差矩阵。\n",
    "\tmin_covar：一个浮点数。给出了方差矩阵对角线上元素的最小值，用于防止过拟合。\n",
    "\tstartprob_prior：一个数组，形状为(n_components, )。初始状态的先验概率分布。\n",
    "\ttransmat_prior：一个数字，形状为(n_components, n_components )。先验的状态转移矩阵。\n",
    "\talgorithm：一个字符串。指定了Decoder算法。可以为 'viterbi'（维特比算法）或者'map' 。\n",
    "\trandom_state：指定随机数种子。\n",
    "\ttol：指定迭代收敛阈值。\n",
    "\tverbose：指定打印日志。\n",
    "\tparams：一个字符串。控制在训练过程中，哪些参数能够得到更新（你也可以指定它们的组合形式）：\n",
    "\t\t's'：初始概率。\n",
    "\t\t't'：转移概率。\n",
    "\t\t'm'：均值。\n",
    "\t\t'c'：偏差。\n",
    "\tinit_params：一个字符串。控制在训练之前，先初始化哪些参数（你也可以指定它们的组合形式）：\n",
    "\t\t's'：初始概率。\n",
    "\t\t't'：转移概率。\n",
    "\t\t'm'：均值。\n",
    "\t\t'c'：偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个类别的视频数据训练单独的隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "n_hmm=6      # 隐藏节点个数\n",
    "# assert n_hmm<10\n",
    "n_iter=300              # 代最大个数   调大点 >200\n",
    "algorithm='map'     # 解码算法   最大后验概率解码       'viterbi'维特比解码算法\n",
    "tol=0.001           # 相邻两次迭代之间对数似然变化 迭代终止阈值\n",
    "init_params='sc' # stemc  stec-56% sec-57% sc-58% c-51%\n",
    "n_features=220 \n",
    "assert n_features>=num_clusters \n",
    "min_covar=0.1\n",
    "# 迭代算法：EM算法\n",
    "#-----------高斯分布---------\n",
    "# model_walk = hmm.GaussianHMM(n_components=n_hmm,covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_box = hmm.GaussianHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params)\n",
    "# model_handclap= hmm.GaussianHMM(n_components=n_hmm, covariance_type=\"full\",n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params)\n",
    "# model_jog = hmm.GaussianHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params)\n",
    "# model_run = hmm.GaussianHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params)\n",
    "# model_handwave = hmm.GaussianHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params)\n",
    "#--------混合高斯分布------------\n",
    "# model_walk = hmm.GMMHMM(n_components=n_hmm,covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_box = hmm.GMMHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_handclap= hmm.GMMHMM(n_components=n_hmm, covariance_type=\"full\",n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_jog = hmm.GMMHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_run = hmm.GMMHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "# model_handwave = hmm.GMMHMM(n_components=n_hmm, covariance_type=\"full\", n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,min_covar=min_covar)\n",
    "#--------多项式--------------------\n",
    "\n",
    "model_walk = hmm.CategoricalHMM(n_components=n_hmm, n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "model_box = hmm.CategoricalHMM(n_components=n_hmm,  n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "model_handclap= hmm.CategoricalHMM(n_components=n_hmm, n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "model_jog = hmm.CategoricalHMM(n_components=n_hmm,  n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "model_run = hmm.CategoricalHMM(n_components=n_hmm, n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "model_handwave = hmm.CategoricalHMM(n_components=n_hmm,  n_iter=n_iter,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features)\n",
    "\n",
    "model_walk.fit(data_walk,x_len_walk)\n",
    "model_box.fit(data_box,x_len_box)\n",
    "model_handclap.fit(data_handclap,x_len_handclap)\n",
    "model_jog.fit(data_jog,x_len_jog)\n",
    "model_run.fit(data_run,x_len_run)\n",
    "model_handwave.fit(data_wave,x_len_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制作测试集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制作测试集和训练集\n",
    "train_labels_walk=labels_walk[::2]\n",
    "train_labels_box=labels_box[::2]\n",
    "train_labels_handclap=labels_handclap[::2]\n",
    "train_labels_jog=labels_jog[::2]\n",
    "train_labels_run=labels_run[::2]\n",
    "train_labels_wave=labels_wave[::2]\n",
    "X_train=np.concatenate([X_train_walk,X_train_box,X_train_handclap,X_train_jog,X_train_run,X_train_wave])\n",
    "y_train=np.concatenate([train_labels_walk,train_labels_box,train_labels_handclap,train_labels_jog,train_labels_run,train_labels_wave])\n",
    "test_labels_walk=labels_walk[1::2]\n",
    "test_labels_box=labels_box[1::2]\n",
    "test_labels_handclap=labels_handclap[1::2]\n",
    "test_labels_jog=labels_jog[1::2]\n",
    "test_labels_run=labels_run[1::2]\n",
    "test_labels_wave=labels_wave[1::2]\n",
    "X_test=np.concatenate([X_test_walk,X_test_box,X_test_handclap,X_test_jog,X_test_run,X_test_wave])\n",
    "y_test=np.concatenate([test_labels_walk,test_labels_box,test_labels_handclap,test_labels_jog,test_labels_run,test_labels_wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试训练集的分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "total_count = len(X_train)\n",
    "for i, x in enumerate(X_train):\n",
    "    # 计算两个类别模型预测的概率\n",
    "    score=[model_walk.score(x.reshape(-1, 1)),model_box.score(x.reshape(-1, 1)),model_handclap.score(x.reshape(-1, 1)),\n",
    "           model_jog.score(x.reshape(-1, 1)),model_run.score(x.reshape(-1, 1)),model_handwave.score(x.reshape(-1, 1))]\n",
    "    \n",
    "    predicted_label=score.index(max(score))\n",
    "    if predicted_label == y_train[i]:\n",
    "        correct_count += 1\n",
    "\n",
    "# 计算分类器的准确率\n",
    "train_accuracy = correct_count / total_count\n",
    "print(\"分类器在训练集的准确率：\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试测试集的分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上进行预测\n",
    "correct_count = 0\n",
    "total_count = len(X_test)\n",
    "predict_labels_test=[]\n",
    "for i, x in enumerate(X_test):\n",
    "    # 计算两个类别模型预测的概率\n",
    "    score=[model_walk.score(x.reshape(-1, 1)),model_box.score(x.reshape(-1, 1)),model_handclap.score(x.reshape(-1, 1)),\n",
    "           model_jog.score(x.reshape(-1, 1)),model_run.score(x.reshape(-1, 1)),model_handwave.score(x.reshape(-1, 1))]\n",
    "    #print(score)           #对数似然值\n",
    "    predicted_label=score.index(max(score))\n",
    "    #print(predicted_label)\n",
    "    predict_labels_test.append(predicted_label)\n",
    "    if predicted_label == y_test[i]:\n",
    "        correct_count += 1\n",
    "# 计算分类器的准确率\n",
    "test_accuracy = correct_count / total_count\n",
    "print(\"分类器在测试集的准确率：\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predict_labels_test)\n",
    "\n",
    "# 绘制热力图\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['walk', 'box', 'handclap', 'jog', 'run', 'handwave'], yticklabels=['walk', 'box', 'handclap', 'jog', 'run', 'handwave'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix in M={num_clusters},N={n_hmm}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态时间规划(DTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def dtw_distance(x1, x2):\n",
    "    # 计算动态时间规整距离\n",
    "    alignment = fastdtw(x1, x2)[1]\n",
    "    dtw_dist = 0\n",
    "    for i, j in alignment:\n",
    "        dtw_dist += abs(x1[i] - x2[j])\n",
    "    return dtw_dist\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return y_pred\n",
    "    def _predict(self, x):\n",
    "        # 计算样本与所有训练数据的DTW距离\n",
    "        distances = [dtw_distance(x, x_train) for x_train in self.X_train]\n",
    "        # 根据距离排序，找到K个最近邻的样本\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # 计算距离的倒数，作为权重\n",
    "        weights = [1 / (distances[i] + 1e-6) for i in k_indices]\n",
    "        # 通过加权投票确定新样本的类别\n",
    "        weighted_votes = defaultdict(float)\n",
    "        for label, weight in zip(k_nearest_labels, weights):\n",
    "            weighted_votes[label] += weight\n",
    "\n",
    "        most_common = max(weighted_votes, key=weighted_votes.get)\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#制作训练集和测试集\n",
    "data_train=data_all[::2]\n",
    "data_test=data_all[1::2]\n",
    "data_hmm=np.concatenate([i for i in data_train]).reshape(-1,1)\n",
    "frames_train=frames[::2]\n",
    "frames_test=frames[1::2]\n",
    "labels_train=labels[::2]\n",
    "labels_test=labels[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于DTW的KNN分类模型对观测序列做分类判别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用手写KNN算法进行分类\n",
    "k=40\n",
    "knn_dtw = KNN(k=k)\n",
    "\n",
    "# 将训练样本和标签转换为 NumPy 数组\n",
    "labels_train = np.array(labels_train)\n",
    "train = np.concatenate([X_train_walk, X_train_box, X_train_handclap, X_train_jog, X_train_run, X_train_wave])\n",
    "test = np.concatenate([X_test_walk, X_test_box, X_test_handclap, X_test_jog, X_test_run, X_test_wave])\n",
    "\n",
    "# 重新训练 KNN 模型\n",
    "knn_dtw.fit(train, labels_train)\n",
    "\n",
    "predictions_train = knn_dtw.predict(train)\n",
    "predictions = knn_dtw.predict(test)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个包含所有视频数据的隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hmm_=23\n",
    "n_iter_=350\n",
    "algorithm='map'\n",
    "tol=0.001\n",
    "n_features_=260\n",
    "model_hmm = hmm.CategoricalHMM(n_components=n_hmm_, n_iter=n_iter_,algorithm=algorithm,verbose=True,tol=tol,init_params=init_params,n_features=n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查序列长度\n",
    "assert sum(frames_train)==len(data_hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "model_hmm.fit(data_hmm,frames_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对观测序列进行解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decodered=[]\n",
    "algorithm_decode='map'\n",
    "for i, x in enumerate(data_train):    \n",
    "    train_decodered.append(model_hmm.decode(x.reshape(-1,1),frames_train[i],algorithm=algorithm_decode)[1])\n",
    "test_decodered=[]\n",
    "for i, x in enumerate(data_test):    \n",
    "    test_decodered.append(model_hmm.decode(x.reshape(-1,1),frames_test[i],algorithm=algorithm_decode)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于DTW的KNN分类模型对解码序列做分类判别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 使用手写KNN算法进行分类\n",
    "k=40\n",
    "knn = KNeighborsClassifier(k=k)\n",
    "# 将训练样本和标签转换为 NumPy 数组\n",
    "train_decodered = np.array(train_decodered)\n",
    "labels_train = np.array(labels_train)\n",
    "\n",
    "knn.fit(train_decodered, labels_train)\n",
    "\n",
    "predictions_train = knn.predict(train_decodered)\n",
    "predictions = knn.predict(test_decodered)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算模型在训练集和测试集的准确率并绘制混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "count=0\n",
    "total_count=len(predictions_train)\n",
    "for i,prediction in enumerate(predictions_train):\n",
    "    if prediction==labels_train[i]:\n",
    "        count+=1\n",
    "train_accuracy=count/total_count\n",
    "print(f\"在训练集上的Accuracy: {train_accuracy}\")\n",
    "\n",
    "count=0\n",
    "total_count=len(predictions)\n",
    "for i,prediction in enumerate(predictions):\n",
    "    if prediction==labels_test[i]:\n",
    "        count+=1\n",
    "test_accuracy=count/total_count\n",
    "print(f\"在测试集上的Accuracy: {test_accuracy}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(labels_test,predictions)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['walk', 'box', 'handclap', 'jog', 'run', 'handwave'], yticklabels=['walk', 'box', 'handclap', 'jog', 'run', 'handwave'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix in M={num_clusters_},N={n_hmm_}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
